{"cells":[{"cell_type":"code","source":["#Object Detection transfer learning training using tensorflow mobilenetssdlitev2 model on Azure ML"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Setup workspace\n","import azureml.core\n","from azureml.core import Workspace\n","print(azureml.core.VERSION)"],"outputs":[],"execution_count":null,"metadata":{"tags":[]}},{"cell_type":"code","source":["# Include your subscription, resource_group and the workspace details which will be used for experiment runs. This will create the workspace, if it doesn't exist\n","subscription_id = ''\n","resource_group = ''\n","workspace_name = ''\n","\n","'''For creating new work space\n","# create aml workspace or create it azure portal\n","#https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py#workspace\n","from azureml.core.authentication import InteractiveLoginAuthentication\n","interactive_auth = InteractiveLoginAuthentication(tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\")\n","\n","from azureml.core import Workspace\n","ws = Workspace.create(name=workspace_name,\n","                      subscription_id=subscription_id,\n","                      resource_group=resource_group,\n","                      create_resource_group=True,\n","                      location='westus',\n","                      auth=InteractiveLoginAuthentication\n","                     )\n","'''"],"outputs":[],"execution_count":null,"metadata":{"tags":[]}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["\n","from azureml.core.authentication import InteractiveLoginAuthentication\n","interactive_auth = InteractiveLoginAuthentication(tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\")\n","ws = Workspace(subscription_id=\"\",\n","               resource_group=\"\",\n","               workspace_name=\"\",\n","               auth=interactive_auth)\n","ws.get_details()\n","print (ws)"]},{"cell_type":"code","source":["# setup datastore for loading custom labelled datasets\n","# For this training the data set was labelled using https://github.com/Microsoft/VoTT tool\n","ds = ws.get_default_datastore()\n","ds.name"],"outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":["# Load data folder to default storage datastore\n","ds.upload(\n","    src_dir='./upload_data',\n","    target_path='tfdata',\n","    overwrite=True)"],"outputs":[],"execution_count":null,"metadata":{"tags":[]}},{"cell_type":"code","source":["\"\"\"\n","# train remote VM - gpu\n","from azureml.core.compute import AmlCompute, ComputeTarget\n","from azureml.core.compute_target import ComputeTargetException\n","\n","try:\n","    compute_target = ComputeTarget(workspace=ws, name='dw-gpu')\n","    print('found existing:', compute_target.name)\n","except ComputeTargetException:\n","    print('creating new.')\n","    compute_config = AmlCompute.provisioning_configuration(\n","        vm_size='STANDARD_NC6',\n","        min_nodes=0,\n","        max_nodes=1)\n","    compute_target = ComputeTarget.create(ws, 'dw-gpu', compute_config)\n","    compute_target.wait_for_completion(show_output=True)\n","\"\"\""],"outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":["# train remote VM - cpu\n","from azureml.core.compute import AmlCompute, ComputeTarget\n","from azureml.core.compute_target import ComputeTargetException\n","\n","try:\n","    compute_target = ComputeTarget(workspace=ws, name='dw-cpu')\n","    print('found existing:', compute_target.name)\n","except ComputeTargetException:\n","    print('creating new.')\n","    compute_config = AmlCompute.provisioning_configuration(\n","        vm_size='STANDARD_D3_V2',\n","        min_nodes=0,\n","        max_nodes=1)\n","    compute_target = ComputeTarget.create(ws, 'dw-cpu', compute_config)\n","    compute_target.wait_for_completion(show_output=True)"],"outputs":[],"execution_count":null,"metadata":{"tags":[]}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","source":["# Mounting the uploaded data for training\n","from azureml.core import Datastore\n","from azureml.core.runconfig import DataReferenceConfiguration\n","\n","ds = Datastore.get(ws, datastore_name=\"workspaceblobstore\")\n","\n","dr_conf = DataReferenceConfiguration(\n","    datastore_name=ds.name,\n","    path_on_datastore='tfdata',\n","    #path_on_compute = '/tfdata'\n","    mode='mount') # or 'download'"],"outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":["\n","'''\n","#Running on remote compute\n","\n","# data reference using storage download (optional)\r\n","from azureml.core import Datastore\r\n","from azureml.core.runconfig import DataReferenceConfigurat\n","ion\r\n","\r\n","ds = Datastore.get(ws, datastore_name=\"workspaceblobstore\")\r\n","\r\n","dr_conf = DataReferenceConfiguration(\r\n","    datastore_name=ds.name,\r\n","    path_on_datastore=''/tfdata'\r\n","    mode='download') #\n","''' 'mount'\r\n","\"\"\""],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["#For local runs\n","from azureml.core import Datastore\n","from azureml.core.runconfig import DataReferenceConfiguration\n","\n","ds = Datastore.get(ws, datastore_name=\"workspaceblobstore\")\n","\n","dr_conf = DataReferenceConfiguration(\n","    datastore_name=ds.name,\n","    path_on_datastore='tfdata',\n","    mode='download') # or 'mount'\n","print(ds)\n","print(dr_conf)\n","print(ds.as_download())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","from azureml.train.estimator import Estimator \n","params= {'--data_folder' : ds.as_mount() } \n","\n","estimator = Estimator(source_directory='./script', \n","                      entry_script='train.py',\n","                      compute_target=compute_target,\n","                      script_params=params,\n","                      node_count=1,\n","                      process_count_per_node=1,\n","                      pip_requirements_file = 'requirements.txt', # pip packages\n","                      custom_docker_image='ACR_NAME.azurecr.io/dw-tf-od1:v2', # using public docker hub\n","                      use_gpu=False)\n"]},{"cell_type":"code","source":["from azureml.core import Experiment\n","\n","experiment_name = 'dw-exp-ssdv2lite'\n","experiment = Experiment(ws, name=experiment_name)\n","\n","run = experiment.submit(estimator)\n","print(run)\n","\n","run.wait_for_completion(show_output=True)"],"outputs":[],"execution_count":null,"metadata":{"tags":["outputPrepend"]}},{"cell_type":"code","source":["trained_model_path = os.getcwd()+'/outputs'\r\n","print(trained_model_path)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Download the retrained frozen graph model (frozen_inference_graph.pb) and pipeline.config created from transfer learning experiment to the outputs folder. Time to download varies based on the size of the model.  \r\n","trained_model_path = os.getcwd()+'/outputs'\r\n","run.download_file(name = 'outputs/frozen_model/frozen_inference_graph.pb', output_file_path = trained_model_path)\r\n","run.download_file(name = 'outputs/frozen_model/pipeline.config', output_file_path = trained_model_path)\r\n","print(trained_model_path)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Register the trained model. Once register you'll find the model in the Models section on the left pane\r\n","from azureml.core.model import Model\r\n","\r\n","model = Model.register(model_path = trained_model_path,\r\n","                      model_name = \"Forks_Scissors_ssdv2lite\",\r\n","                      tags = {\"data\": \"ssd_mobilenetv2lite\", \"model\": \"object_detection\", \"type\": \"ssd_mobilenetv2lite\"},\r\n","                      description = \"Retrained Forks Scissors based on ssd_mobilenetv2lite\",\r\n","                      workspace = ws)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["'''\r\n","### Convert the trained model to IR -> Blob using Intel Openvino toolkit for running on Devkit running Myriadx chipset\r\n","1) Setup Intel openvino toolkit on local machine https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_windows.html\r\n","2) Download the Frozen_inference_graph.pb model file, pipeline.config, labels.txt and config.json from the notebook vm /outputs folder to local machine for model conversion\r\n","3) Model conversion to IR and Blob. Run the command from the command prompt as administrator\r\n","Pb->IR\r\n","python \"c:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\model_optimizer\\mo_tf.py\"  --input_model frozen_inference_graph.pb --tensorflow_object_detection_api_pipeline_config pipeline.config --transformations_config   \"C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\model_optimizer\\extensions\\front\\tf\\ssd_v2_support.json\" \r\n","\r\n","IR->Blob\r\n","\"C:\\Program Files (x86)\\IntelSWTools\\openvino_2020.3.194\\deployment_tools\\inference_engine\\bin\\intel64\\Release\\myriad_compile.exe\" -m frozen_inference_graph.xml -o fast-rcnn-resnet50.blob -VPU_MYRIAD_PLATFORM VPU_MYRIAD_2480 -VPU_NUMBER_OF_SHAVES 8 -VPU_NUMBER_OF_CMX_SLICES 8 -iop \"image_tensor:U8, image_info:FP32\" -op FP32\r\n","4) If you trained with your custom data set, make sure to update the labels.txt with the classes used for training\r\n","5) Zip the converted model file .blob, labels.txt and config.json to model.zip and upload the zip file to /ouputs/convertedmodel folder in notebook vm\r\n","6) Next step is to upload the zip file to blobstore and push it to the devkit using module twin update\r\n","'''"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Setup workspace\n","import azureml.core\n","from azureml.core import Workspace"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["from azureml.core.authentication import InteractiveLoginAuthentication\n","interactive_auth = InteractiveLoginAuthentication(tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\")\n","ws = Workspace(subscription_id=\"7e0a3a6c-0601-4e22-b727-e1c85bfaa678\",\n","               resource_group=\"ADEResource_Tony\",\n","               workspace_name=\"dw-ws\",\n","               auth=interactive_auth)\n","ws.get_details()\n","print (ws)"]},{"cell_type":"code","source":["# get the default datastore\r\n","ds = ws.get_default_datastore()\r\n","print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":[]}},{"cell_type":"code","source":["#set data path for model.zip\r\n","data_path = 'modelpath'\r\n","ds.upload(src_dir='./outputs/convertedmodel', target_path=data_path, overwrite=True)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":[]}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["#Generate saas url for module twin update\n","from azure.storage.blob.baseblobservice import BaseBlobService,ContainerPermissions,BlobPermissions\n","from datetime import datetime, timedelta\n","AZURE_ACC_NAME = ds.account_name\n","AZURE_PRIMARY_KEY = ds.account_key\n","AZURE_CONTAINER = ds.container_name\n","AZURE_BLOB=ds.name\n","AZURE_File=data_path+'/model_ssdv2l.zip'\n","service = BaseBlobService(account_name=AZURE_ACC_NAME, account_key=AZURE_PRIMARY_KEY)\n","sas_url  = service.generate_blob_shared_access_signature(AZURE_CONTAINER,AZURE_File,permission=BlobPermissions.READ,expiry= datetime.utcnow() + timedelta(hours=48))\n","downloadurl ='https://'+AZURE_ACC_NAME+'.blob.core.windows.net/'+AZURE_CONTAINER+'/'+AZURE_File+'?'+sas_url\n","print('https://'+AZURE_ACC_NAME+'.blob.core.windows.net/'+AZURE_CONTAINER+'/'+AZURE_File+'?'+sas_url)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["pip install azure-storage-blob==2.1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["#Perform Module twin update\n","#Incorporate the connection string, device_id and the module_id values from your IoTHub\n","\n","!pip install azure-iot-hub\n","import sys\n","from azure.iot.hub import IoTHubRegistryManager\n","from azure.iot.hub.models import Twin, TwinProperties\n","\n","#Incorporate Iothub connection string and the default module name\n","#Go to Https://portal.azure.com\n","#Select your IoTHub\n","#Click on Shared access policies\n","#click service on right\n","#Copy the iothub connection string primary key\n","\n","CONNECTION_STRING = \"\"\n","DEVICE_ID = ''\n","MODULE_ID = \"azureeyemodule\"\n","\n","try:\n","    # RegistryManager\n","    iothub_registry_manager = IoTHubRegistryManager(CONNECTION_STRING)\n","\n","    module_twin = iothub_registry_manager.get_module_twin(DEVICE_ID, MODULE_ID)\n","    print ( \"\" )\n","    print ( \"Module twin properties before update    :\" )\n","    print ( \"{0}\".format(module_twin.properties) )\n","\n","    # Update twin\n","    twin_patch = Twin()\n","    twin_patch.properties = TwinProperties(desired={\"ModelZipUrl\": downloadurl})\n","    updated_module_twin = iothub_registry_manager.update_module_twin(\n","        DEVICE_ID, MODULE_ID, twin_patch, module_twin.etag\n","    )\n","    print ( \"\" )\n","    print ( \"Module twin properties after update     :\" )\n","    print ( \"{0}\".format(updated_module_twin.properties) )\n","\n","except Exception as ex:\n","    print ( \"Unexpected error {0}\".format(ex) )\n","except KeyboardInterrupt:\n","    print ( \"IoTHubRegistryManager sample stopped\" )"]},{"cell_type":"code","source":["#Environment Cleanup\r\n","#Check the docker image id created for the acr repo name\r\n","#!docker images"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Delete docker registry by uncommenting the below step and including the image id\r\n","#!docker rmi -f <imageid>"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# delete cpu compute\n","\"\"\"\n","mycompute = AmlCompute(workspace=ws, name='dw-cpu1')\n","mycompute.delete()\n","\n","# delete gpu compute\n","mycompute = AmlCompute(workspace=ws, name='dw-gpu')\n","mycompute.delete()\n","\"\"\""],"outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":["# delete workspace\n","#ws.delete(delete_dependent_resources=True)\n"],"outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"kernelspec":{"name":"python_defaultSpec_1596468567634","language":"python","display_name":"Python 3.7.7 64-bit (conda)"},"language_info":{"name":"python","version":"3.6.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"nteract":{"version":"nteract-front-end@1.0.0"},"kernel_info":{"name":"python3-azureml"}},"nbformat":4,"nbformat_minor":4}
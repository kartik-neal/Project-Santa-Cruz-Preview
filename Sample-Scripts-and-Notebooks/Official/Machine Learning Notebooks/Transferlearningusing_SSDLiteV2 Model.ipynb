{"cells":[{"cell_type":"markdown","source":["Copyright (c) Microsoft Corporation. All rights reserved.\n","Licensed under the MIT License.\n","\n","# Train your own Model and Deploy to Device\n","\n","**NOTE**\n","* Warning: copying *.pb, *.bin, or, *.blob using the web interface can corrupt the files. If needed download and use Azure storage explorer or the CL.\n","* Make sure to run each cell individually as some cells will require input parameters.\n","\n","This notebook shows how to create a Tensorflow object detection model, how to convert the model to the appropriate format for the Eye development\n","kit, and how to deploy the model to your kit.\n","\n","This notebook takes a transfer learning approach, using a pre-trained Tensorflow Mobilenet model with custom dataset\n","and SSDLite layers that we will train to detect bowls.\n","We use the [Tensorflow object detection API](https://github.com/tensorflow/models/tree/master/research/object_detection).\n","\n","The trained model will be deployed to Azure Eye Devkit using Module Twin Update method."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["# Setting up the GPU for Tensorflow 1.15\n","\n","We will be using Tensorflow 1.15 for this example, and Tensorflow 2.x examples should be coming soon.\n","\n","The datascience notebook VMs do not come with the appropriate libraries installed for this version of Tensorflow, and so we will need to\n","install them. If your compute node/cluster is a GPU machine, you will want to make sure the appropriate version of CUDA is installed\n","into your VM so that Tensorflow can use it to accelerate training substantially.\n","\n","If you are using a GPU-enabled compute device, you will want to follow these steps. Otherwise, feel free to skip this cell.\n","\n","1. Start your compute instance if it is not already started.\n","1. Select Open Terminal, which should be a button next to your compute and kernel selection boxes.\n","1. Run `conda info -e` to list the available conda environments. There should be an azureml_py36 environment.\n","1. Select azureml_py36 by running `conda activate azureml_py36`\n","1. Uninstall Pytorch and Tensorflow (which depend on the wrong version of CUDA) and the wrong version of CUDA with `conda uninstall cudatoolkit pytorch tensorflow`\n","1. Install the correct version of CUDA with `conda install cudatoolkit=10.0`\n","\n","You may now close the terminal tab. We will install Tensorflow in a cell of this notebook later."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["## Collecting the data\n","\n","In this notebook, we use a custom dataset that will be used to train a model that detects bowls.\n","\n","To compile this dataset, we have a couple of options. First, we could use a labeling tool like [VoTT](https://github.com/microsoft/VoTT) or\n","[LabelImg](https://github.com/tzutalin/labelImg). Just make sure to export the dataset to Pascal VOC format. There should be one XML file for\n","each image, and the XML files should be under annotations/xmls.\n","\n","The other option for compiling a dataset is to use an already existing dataset like COCO and then filter out all the images and annotations\n","that we don't care about. This is the approach we take here.\n","\n","**NOTE:**\n","The first cell in this notebook contains the code necessary to get the dataset and convert it to the format we need. This will involve downloading\n","the entire COCO dataset (train and validation splits for 2017), which is tens of GBs in size. This may not fit in your Azure ML compute's storage.\n","Our recommendation is to run this cell locally (i.e., copy and paste the commands into your own shell and run them), then upload\n","the resulting dataset to Azure via [Azure Storage Explorer](https://azure.microsoft.com/en-us/features/storage-explorer/)."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# First, get the Santa Cruz repository\n","# **Note** You cannot currently wget the file from GitHub, as it is still private.\n","# You should instead go and download the file manually by going to https://github.com/microsoft/Project-Santa-Cruz-Private-Preview\n","# and downloading the .zip file of the project.\n","#!wget https://github.com/microsoft/Project-Santa-Cruz-Private-Preview/archive/main.zip\n","#!unzip Project-Santa-Cruz-Private-Preview-main.zip\n","#!rm Project-Santa-Cruz-Private-Preview-main.zip\n","#!cd Project-Santa-Cruz-Private-Preview-main/Sample-Scripts-and-Notebooks/Official/Scripts\n","\n","# Get the data\n","#!mkdir -p coco/images\n","\n","# Get the 2017 training split\n","#!wget http://images.cocodataset.org/zips/train2017.zip\n","#!unzip train2017.zip\n","#!rm train2017.zip\n","#!mv train2017 coco/images/\n","\n","# Get the 2017 validation split\n","#!wget http://images.cocodataset.org/zips/val2017.zip\n","#!unzip val2017.zip\n","#!rm val2017.zip\n","#!mv val2017 coco/images/\n","\n","# Get all the annotations\n","#!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n","#!unzip annotations_trainval2017.zip\n","#!rm annotations_trainval2017.zip\n","#!mv annotations coco/annotations\n","\n","# Filter the annotations down to a JSON that only contains images that contain at least one bowl\n","# python filter_coco.py -i coco/annotations/instances_train2017.json -o coco_train2017_bowls.json -c bowl\n","# python filter_coco.py -i coco/annotations/instances_val2017.json -o coco_val2017_bowls.json -c bowl\n","\n","# Now create a filtered version of the coco train and val split based on their annotation JSONs\n","# python create_coco_subset.py -i coco_train2017_bowls.json coco_val2017_bowls.json -g coco/images/train2017 coco/images/val2017 -o coco_filtered\n","\n","# Create Pascal VOC formatted dataset from the COCO subset\n","# python convert_coco_to_voc.py coco_filtered --target bowls_voc\n","\n","# You should now have a data folder called bowls_voc, which should look like this:\n","# bowls_voc/\n","#   - annotations/\n","#       - xmls/\n","#   - images/\n","\n","# Now upload 'bowls_voc' to your workspace using the Azure Storage Explorer.\n","# To do so, please follow these instructions:\n","# 1. Install Azure Storage Explorer and set it up according to the instructions found here: https://azure.microsoft.com/en-us/features/storage-explorer/\n","# 2. Once your account is linked, you should be able to open Azure Storage Explorer, select the appropriate subscription, storage account, and file share.\n","# 3. Upload 'bowls_voc' to the /Users/<your user> folder under the appropriate file share."],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599082546298}}},{"cell_type":"code","source":["# Save current directory for later reference\n","modelroot = !pwd\n","modelroot = modelroot[0]\n","modelroot"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599082546638}}},{"cell_type":"code","source":["# Setup workspace for Azure ML\n","!pip install azureml.core\n","\n","import azureml.core\n","from azureml.core import Workspace\n","print(azureml.core.VERSION)"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1599082560851},"tags":[]}},{"cell_type":"code","source":["# Install tensorflow v1.15 required for OpenVINO model conversion\n","!pip install tensorflow-gpu==1.15"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599082563544}}},{"cell_type":"code","source":["# Install Tensor flow models and scripts, v1.13.0 needed for OpenVINO, which is what we use on the eye development kit.\r\n","repository = '--depth 1 --branch v1.13.0 https://github.com/tensorflow/models.git'\r\n","!pip install tf-slim\r\n","!git clone $repository"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599082566514},"tags":[]}},{"cell_type":"code","source":["# Install required TF packages\r\n","!sudo -s apt-get install -qq protobuf-compiler python-tk\r\n","!pip install Cython contextlib2 pillow lxml matplotlib PyDrive pycocotools build utils dataclasses install azure-iot-device azure-iot-h numpy==1.17ub"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599082572881},"tags":[]}},{"cell_type":"code","source":["# Setup python path for TF object detection API and TF-Slim\r\n","import os\r\n","import sys\r\n","\r\n","cwd = os.getcwd()\r\n","sys.path.append(cwd)\r\n","\r\n","research = cwd + '/models/research'\r\n","sys.path.append(research)\r\n","\r\n","slim = cwd + '/models/research/slim'\r\n","sys.path.append(slim)\r\n","\r\n","%set_env PYTHONPATH=''\r\n","os.environ['PYTHONPATH'] = research + \":\" + slim +  \":\" + cwd\r\n","os.environ['PYTHONPATH']"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599082573053},"tags":[]}},{"cell_type":"code","source":["# Update protocol buffers for TF object detection API\r\n","!curl -OL 'https://github.com/google/protobuf/releases/download/v3.2.0/protoc-3.2.0-linux-x86_64.zip'\r\n","\r\n","# Unzip it\r\n","unzipcmd = '-o protoc-3.2.0-linux-x86_64.zip -d protoc3'\r\n","!unzip $unzipcmd\r\n","\r\n","# Put it into /usr/local/bin to put it in the system path\r\n","movecmd = 'mv protoc3/bin/* /usr/local/bin/'\r\n","!sudo $movecmd\r\n","\r\n","# Add header files to the linker's include path\r\n","movecmd = 'mv protoc3/include/* /usr/local/include/'\r\n","!sudo $movecmd"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599082576758},"tags":[]}},{"cell_type":"code","source":["# Jump into the tensorflow object detection API research directory\r\n","researchfolder =  modelroot + '/models/research'\r\n","%cd $researchfolder\r\n","\r\n","# As per their installation instructions, compile everything in the protos folder using protoc\r\n","protoccmd = '/usr/local/bin/protoc ' + 'object_detection/protos/*.proto --python_out=.'\r\n","!$protoccmd"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599082578604},"tags":[]}},{"cell_type":"code","source":["# Check Tensorflow version. OpenVINO currently requires TF 1.x, so that's what we use\r\n","import tensorflow.compat.v1 as tf\r\n","print(tf.__version__)\r\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599082596459},"tags":[]}},{"cell_type":"code","source":["# Note: Cycle jupyter kernel if needed to pickup tensorflow change from 2.1 to 1.15.0, then run the previous cell again."],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599082596625}}},{"cell_type":"code","source":["# Run setup for TF object detection API\r\n","args = researchfolder + '/setup.py build'\r\n","!python $args\r\n","\r\n","# Install TF object detection API\r\n","args = researchfolder + '/setup.py install'\r\n","!python $args"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599082728542},"tags":["outputPrepend"]}},{"cell_type":"code","source":["# Run tensorflow model builder test just to make sure we have TF object detection API set up correctly\r\n","args = modelroot + '/models/research/object_detection/builders/model_builder_test.py'\r\n","!python $args"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599082743612},"tags":[]}},{"cell_type":"code","source":["# Create data folder for training dataset and model\n","%cd $modelroot\n","!mv $modelroot/bowls_voc $modelroot/data\n","%cd data"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599082744040},"tags":[]}},{"cell_type":"code","source":["# This is a little wonky, but we are later on using a script from the TF object detection API,\r\n","# create_pet_tf_record.py to generate a TF record. This script requires certain things in the\r\n","# dataset that we don't have, and which we don't use. We create those things here.\r\n","from PIL import Image\r\n","\r\n","image_files = os.listdir('images')\r\n","im_files = [os.path.splitext(x)[0] for x in image_files]\r\n","with open('annotations/trainval.txt', 'w') as text_file:\r\n","  for row in im_files:\r\n","    text_file.write(row + '\\n')\r\n","\r\n","%cd ./annotations\r\n","!mkdir trimaps\r\n","\r\n","image = Image.new('RGB', (640, 480))\r\n","for fname in os.listdir(\"xmls\"):\r\n","  fname, _ = os.path.splitext(fname)\r\n","  image.save(os.path.join(\"trimaps\", fname + \".png\"))"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599083076606},"tags":[]}},{"cell_type":"code","source":["# Create category labels file for Tensorflow training and validation files (label map)\r\n","label_map_fpath = modelroot + '/models/research/object_detection/data/bowls.pbtxt'\r\n","print(label_map_fpath)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599083076894},"tags":[]}},{"cell_type":"code","source":["%%writefile $label_map_fpath\r\n","item {\r\n","  id: 1\r\n","  name: 'bowl'\r\n","}"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":[]}},{"cell_type":"code","source":["# Fix to remove ^M nonprintable char from end of string lines in file created above\r\n","# to see issue run \"!cat -v $label_map_fname\" before and after fix\r\n","with open(label_map_fpath, 'r') as file:\r\n","    label_map_file = file.read()\r\n","update_file = open(label_map_fpath, \"w\")\r\n","update_file.writelines(label_map_file)\r\n","update_file.close() "],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599083077182}}},{"cell_type":"code","source":["# Create Tensorflow training data record files by co-opting this create_pet_tf_record.py script for our purposes\r\n","%cd $modelroot/data\r\n","\r\n","script = modelroot + \"/models/research/object_detection/dataset_tools/create_pet_tf_record.py\"\r\n","args = f\"{script} --label_map_path={label_map_fpath} --data_dir=./ --output_dir=./ --num_shards=1\"\r\n","!python $args\r\n","\r\n","# Now update the names of the output files\r\n","!mv pet_faces_train.record-00000-of-00001 tf_train.record\r\n","!mv pet_faces_val.record-00000-of-00001 tf_val.record"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599083819340},"tags":[]}},{"cell_type":"code","source":["# Download pretrained model for transfer learning: SSD Lite MobileNet V2 COCO\r\n","!curl -OL 'http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz'\r\n","model_file = os.path.join(modelroot, \"data\", \"ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\")"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599083820953},"tags":[]}},{"cell_type":"code","source":["# Uncompress model\r\n","import os\r\n","import shutil\r\n","import glob\r\n","import urllib\r\n","import tarfile\r\n","import urllib.request\r\n","\r\n","tar = tarfile.open(model_file)\r\n","tar.extractall()\r\n","tar.close()"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599083823171}}},{"cell_type":"code","source":["# Prepare model config file paths and tensorflow configuration file for SSDLiteV2 Model\r\n","config_fname = modelroot + '/models/research/object_detection/samples/configs/ssdlite_mobilenet_retrained.config'\r\n","fine_tune_checkpoint = '\"' + modelroot + '/data/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt' + '\"'\r\n","input_path_train = '\"' + modelroot + '/data/tf_train.record' + '\"'\r\n","label_map_path = '\"' + modelroot + '/models/research/object_detection/data/bowls.pbtxt' + '\"'\r\n","input_path_eval = '\"' + modelroot + '/data/tf_val.record' + '\"'"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599083823456}}},{"cell_type":"code","source":["%%writefile $config_fname\n","model {\n","  ssd {\n","    num_classes: 1\n","    box_coder {\n","      faster_rcnn_box_coder {\n","        y_scale: 10.0\n","        x_scale: 10.0\n","        height_scale: 5.0\n","        width_scale: 5.0\n","      }\n","    }\n","    matcher {\n","      argmax_matcher {\n","        matched_threshold: 0.5\n","        unmatched_threshold: 0.5\n","        ignore_thresholds: false\n","        negatives_lower_than_unmatched: true\n","        force_match_for_each_row: true\n","      }\n","    }\n","    similarity_calculator {\n","      iou_similarity {\n","      }\n","    }\n","    anchor_generator {\n","      ssd_anchor_generator {\n","        num_layers: 6\n","        min_scale: 0.2\n","        max_scale: 0.95\n","        aspect_ratios: 1.0\n","        aspect_ratios: 2.0\n","        aspect_ratios: 0.5\n","        aspect_ratios: 3.0\n","        aspect_ratios: 0.3333\n","      }\n","    }\n","    image_resizer {\n","      fixed_shape_resizer {\n","        height: 300\n","        width: 300\n","      }\n","    }\n","    box_predictor {\n","      convolutional_box_predictor {\n","        min_depth: 0\n","        max_depth: 0\n","        num_layers_before_predictor: 0\n","        use_dropout: false\n","        dropout_keep_probability: 0.8\n","        kernel_size: 3\n","        use_depthwise: true\n","        box_code_size: 4\n","        apply_sigmoid_to_scores: false\n","        conv_hyperparams {\n","          activation: RELU_6,\n","          regularizer {\n","            l2_regularizer {\n","              weight: 0.00004\n","            }\n","          }\n","          initializer {\n","            truncated_normal_initializer {\n","              stddev: 0.03\n","              mean: 0.0\n","            }\n","          }\n","          batch_norm {\n","            train: true,\n","            scale: true,\n","            center: true,\n","            decay: 0.9997,\n","            epsilon: 0.001,\n","          }\n","        }\n","      }\n","    }\n","    feature_extractor {\n","      type: 'ssd_mobilenet_v2'\n","      min_depth: 16\n","      depth_multiplier: 1.0\n","      use_depthwise: true\n","      conv_hyperparams {\n","        activation: RELU_6,\n","        regularizer {\n","          l2_regularizer {\n","            weight: 0.00004\n","          }\n","        }\n","        initializer {\n","          truncated_normal_initializer {\n","            stddev: 0.03\n","            mean: 0.0\n","          }\n","        }\n","        batch_norm {\n","          train: true,\n","          scale: true,\n","          center: true,\n","          decay: 0.9997,\n","          epsilon: 0.001,\n","        }\n","      }\n","    }\n","    loss {\n","      classification_loss {\n","        weighted_sigmoid {\n","        }\n","      }\n","      localization_loss {\n","        weighted_smooth_l1 {\n","        }\n","      }\n","      hard_example_miner {\n","        num_hard_examples: 3000\n","        iou_threshold: 0.99\n","        loss_type: CLASSIFICATION\n","        max_negatives_per_positive: 3\n","        min_negatives_per_image: 3\n","      }\n","      classification_weight: 1.0\n","      localization_weight: 1.0\n","    }\n","    normalize_loss_by_num_matches: true\n","    post_processing {\n","      batch_non_max_suppression {\n","        score_threshold: 1e-8\n","        iou_threshold: 0.6\n","        max_detections_per_class: 100\n","        max_total_detections: 100\n","      }\n","      score_converter: SIGMOID\n","    }\n","  }\n","}\n","\n","train_config: {\n","  batch_size: 24\n","  optimizer {\n","    rms_prop_optimizer: {\n","      learning_rate: {\n","        exponential_decay_learning_rate {\n","          initial_learning_rate: 0.004\n","          decay_steps: 800720\n","          decay_factor: 0.95\n","        }\n","      }\n","      momentum_optimizer_value: 0.9\n","      decay: 0.9\n","      epsilon: 1.0\n","    }\n","  }\n","  fine_tune_checkpoint: $fine_tune_checkpoint\n","  fine_tune_checkpoint_type:  \"detection\"\n","  # Note: The below line limits the training process to 200K steps, which we\n","  # empirically found to be sufficient enough to train the pets dataset. This\n","  # effectively bypasses the learning rate schedule (the learning rate will\n","  # never decay). Remove the below line to train indefinitely.\n","  num_steps: 200000\n","  data_augmentation_options {\n","    random_horizontal_flip {\n","    }\n","  }\n","  data_augmentation_options {\n","    ssd_random_crop {\n","    }\n","  }\n","}\n","\n","train_input_reader: {\n","  tf_record_input_reader {\n","    input_path: $input_path_train\n","  }\n","  label_map_path: $label_map_path\n","}\n","\n","eval_config: {\n","  num_examples: 8000\n","  # Note: The below line limits the evaluation process to 10 evaluations.\n","  # Remove the below line to evaluate indefinitely.\n","  max_evals: 10\n","}\n","\n","eval_input_reader: {\n","  tf_record_input_reader {\n","    input_path: $input_path_eval\n","  }\n","  label_map_path: $label_map_path\n","  shuffle: false\n","  num_readers: 1\n","}"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":[]}},{"cell_type":"code","source":["# Update config file paths\r\n","with open(config_fname, 'r') as f:\r\n","    config_file = f.read()\r\n","config_file = config_file.replace('$fine_tune_checkpoint', fine_tune_checkpoint)\r\n","config_file = config_file.replace('$label_map_path', label_map_path)\r\n","config_file = config_file.replace('$input_path_train', input_path_train)\r\n","config_file = config_file.replace('$input_path_eval', input_path_eval)\r\n","update_file = open(config_fname, 'w')\r\n","update_file.writelines(config_file)\r\n","update_file.close() \r\n"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599083823786}}},{"cell_type":"code","source":["# Run tensorflow fine tuning training.\r\n","# To train on CPU instead of GPU uncomment the 'CUDA_VISIBLE_DEVICES' line below\r\n","# If training for more than 10,000 iterations the notebook will stop updating output after a while, but the training is still happening in the background\r\n","# In that case run !nvidia-smi to check the current load on the GPU to see if training is still happening\r\n","# You can watch for new ckeckpoint files being written about every 10 minutes with the command !ls $modelroot/data/retrained\r\n","#\r\n","# Note that we train for 30k steps here. This is sufficient to get reasonable results, though you may prefer to train for longer.\r\n","%cd $modelroot\r\n","\r\n","model_dir = os.path.join(modelroot, \"data\", \"retrained\")\r\n","!mkdir $model_dir\r\n","\r\n","import os\r\n","# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n","!python ./models/research/object_detection/model_main.py --pipeline_config_path=$config_fname --model_dir=$model_dir --alsologtostderr --num_train_steps=30000 --num_eval_steps=30000"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599081136949},"tags":["outputPrepend"]}},{"cell_type":"code","source":["# Get the last checkpoint to use for exporting a frozen graph\r\n","import re\r\n","\r\n","%cd $modelroot/data\r\n","\r\n","lst = os.listdir('retrained')\r\n","lf = filter(lambda k: 'model.ckpt-' in k, lst)\r\n","fileList = str(list(lf))\r\n","checkPointNumbers = re.findall(r'[0-9]+', fileList)\r\n","checkPointNumbers = [int(i) for i in checkPointNumbers]  \r\n","last_model = 'model.ckpt-' + str(max(checkPointNumbers))\r\n","print(last_model)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599073934210},"tags":[]}},{"cell_type":"code","source":["# Export frozen graph\r\n","!python $modelroot/models/research/object_detection/export_inference_graph.py --input_type=image_tensor --pipeline_config_path=$config_fname --output_directory=fine_tuned_model --trained_checkpoint_prefix=retrained/$last_model"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599073987537},"tags":["outputPrepend"]}},{"cell_type":"code","source":["# Test inference on photo using frozen graph\n","import tensorflow.compat.v1 as tf\n","import numpy as np\n","import os\n","import cv2\n","from object_detection.utils import label_map_util\n","from object_detection.utils import visualization_utils as vis_util\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","\n","# Path to the frozen graph:\n","PATH_TO_FROZEN_GRAPH = modelroot + '/data/fine_tuned_model/frozen_inference_graph.pb'\n","\n","# Path to the label map\n","PATH_TO_LABEL_MAP = modelroot + '/models/research/object_detection/data/bowls.pbtxt'\n","\n","# Bowl image\n","# Note that because the train/eval split is randomized, it is possible this image was in the train split.\n","IMAGE_PATH = modelroot + '/data/images/bowl_000000084259.jpg'\n","\n","# Number of classes \n","NUM_CLASSES = 1\n","\n","# Minimum confidence value needed to display the bounding box on the image. In range [0.0, 1.0].\n","MIN_THRESHOLD = 0.40\n","\n","# Read the frozen graph\n","detection_graph = tf.Graph()\n","with detection_graph.as_default():\n","    od_graph_def = tf.GraphDef()\n","    with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n","        serialized_graph = fid.read()\n","        od_graph_def.ParseFromString(serialized_graph)\n","        tf.import_graph_def(od_graph_def, name='')\n","\n","label_map = label_map_util.load_labelmap(PATH_TO_LABEL_MAP)\n","categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n","category_index = label_map_util.create_category_index(categories)\n","\n","# Detection\n","with detection_graph.as_default():\n","    with tf.Session(graph=detection_graph) as sess:\n","        # Read in the image and convert BGR to RGB\n","        image_np = cv2.imread(IMAGE_PATH)[:,:,::-1]\n","\n","        # Expand dimensions since the model expects images to have shape: [1, None, None, 3] \n","        image_np_expanded = np.expand_dims(image_np, axis=0)\n","\n","        # Extract image tensor\n","        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n","        \n","        # Extract detection boxes\n","        boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n","        \n","        # Extract detection scores\n","        scores = detection_graph.get_tensor_by_name('detection_scores:0')\n","        \n","        # Extract detection classes\n","        classes = detection_graph.get_tensor_by_name('detection_classes:0')\n","        \n","        # Extract number of detections\n","        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n","\n","        # Actual detection.\n","        boxes, scores, classes, num_detections = sess.run([boxes, scores, classes, num_detections], feed_dict={image_tensor: image_np_expanded})\n","        print(f\"BOXES (shaped {boxes.shape}):\\n{boxes}\")\n","        print(f\"SCORES (shaped {scores.shape}):\\n{scores}\")\n","        print(f\"CLASSES (shaped {classes.shape}):\\n{classes}\")\n","        print(f\"NDETECTIONS (shaped {num_detections.shape}):\\n{num_detections}\")\n","\n","        # Visualization of the results of a detection.\n","        vis_util.visualize_boxes_and_labels_on_image_array(\n","            image_np,\n","            np.squeeze(boxes),\n","            np.squeeze(classes).astype(np.int32),\n","            np.squeeze(scores),\n","            category_index,\n","            use_normalized_coordinates=True,\n","            line_thickness=3,\n","            min_score_thresh=MIN_THRESHOLD\n","            )"],"outputs":[],"execution_count":null,"metadata":{"tags":[],"gather":{"logged":1599074014829}}},{"cell_type":"code","source":["# Display test inference photo\r\n","from matplotlib.pyplot import imshow\r\n","import numpy as np\r\n","from PIL import Image\r\n","\r\n","%matplotlib inline\r\n","plt.figure(figsize = (12, 8))\r\n","imshow(image_np)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":false,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1599074015582}}},{"cell_type":"markdown","source":["## OpenVINO\n","\n","At this point, you should have a model exported from Tensorflow and it should be providing good bounding boxes on bowls.\n","The rest of this notebook will show you how to convert the model into the format that the EyeSOM dev kit requires,\n","and then how to download it to your device."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from azureml.core.authentication import InteractiveLoginAuthentication\n","interactive_auth = InteractiveLoginAuthentication(tenant_id=\"<YOUR TENANT ID>\")\n","\n","# If you have an Azure config.json in your workspace, you can use this line\n","#ws = Workspace.from_config(auth=interactive_auth)\n","\n","# Otherwise, you can manually fill in your information and use the next lines\n","ws = Workspace(subscription_id=\"<YOUR SUBSCRIPTION ID>\",\n","                resource_group=\"<YOUR RESOURCE GROUP>\",\n","                workspace_name=\"<YOUR WORKSPACE NAME>\",\n","                auth=interactive_auth)\n","\n","ws.get_details()\n","print(ws)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":[],"gather":{"logged":1598986131005}}},{"cell_type":"code","source":["# Reload workspace to register model\r\n","from azureml.core import Workspace\r\n","ws.get_details()"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598986351563}}},{"cell_type":"code","source":["# Register the trained model. Once registered, you'll find the model in the Models section on the left pane\r\n","from azureml.core.model import Model\r\n","\r\n","trained_model_path = modelroot + '/data/fine_tuned_model/'\r\n","print(\"Trained model path:\", trained_model_path)\r\n","\r\n","model = Model.register(model_path=trained_model_path,\r\n","                      model_name=\"bowl_ssdv2lite\",\r\n","                      tags={\"data\": \"ssd_mobilenetv2lite\", \"model\": \"object_detection\", \"type\": \"ssd_mobilenetv2lite\"},\r\n","                      description=\"Retrained bowl detection based on ssd_mobilenetv2lite\",\r\n","                      workspace=ws)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598986362581}}},{"cell_type":"code","source":["# Install Intel® Distribution of OpenVINO™ toolkit\r\n","# https://docs.openvinotoolkit.org/2019_R2/_docs_install_guides_installing_openvino_apt.html\r\n","frozen_inference_graph = modelroot + '/data/fine_tuned_model/frozen_inference_graph.pb'\r\n","\r\n","!curl -OL -o GPG-PUB-KEY-INTEL-OPENVINO-2020 'https://apt.repos.intel.com/openvino/2020/GPG-PUB-KEY-INTEL-OPENVINO-2020'\r\n","\r\n","aptKey = 'GPG-PUB-KEY-INTEL-OPENVINO-2020'\r\n","!sudo apt-key add $aptKey\r\n","\r\n","!sudo apt-key list\r\n","\r\n","echo_args = '\"deb https://apt.repos.intel.com/openvino/2020 all main\" | sudo tee /etc/apt/sources.list.d/intel-openvino-2020.list'\r\n","!echo $echo_args\r\n","\r\n","!sudo apt update\r\n","\r\n","# Make sure everything worked. You should see a bunch of intel-openvino stuff in the apt packages\r\n","!apt-cache search openvino\r\n","!sudo apt-cache search intel-openvino-dev-ubuntu16\r\n","\r\n","package = 'intel-openvino-dev-ubuntu16-2020.2.130'\r\n","!sudo apt install -y $package"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598986600563}}},{"cell_type":"code","source":["#Use OpenVINO to converto frozen graph PB to IR format\n","%cd $modelroot/data\n","\n","#import OpenVINO env vars\n","convertCMD = 'source /opt/intel/openvino/bin/setupvars.sh && '\n","\n","#convert frozen graph to Intermediate Representation\n","convertCMD += '/opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py '\n","convertCMD += '--input_model ' + modelroot + '/data/fine_tuned_model/frozen_inference_graph.pb '\n","convertCMD += '--tensorflow_object_detection_api_pipeline_config ' + modelroot + '/data/fine_tuned_model/pipeline.config '\n","convertCMD += '--tensorflow_use_custom_operations_config /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json '\n","convertCMD += '--reverse_input_channels'\n","\n","print(convertCMD)\n","!$convertCMD"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598978236673}}},{"cell_type":"code","source":["# Use OpenVINO to compile IR format to blob\r\n","\r\n","# Import OpenVINO env vars\r\n","convertCMD = 'source /opt/intel/openvino/bin/setupvars.sh && '\r\n","\r\n","# Compile IR to blob \r\n","convertCMD += '/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/myriad_compile '\r\n","convertCMD += '-m frozen_inference_graph.xml '\r\n","convertCMD += '-o ssdlite_mobilenet_v2.blob '\r\n","convertCMD += '-VPU_MYRIAD_PLATFORM VPU_MYRIAD_2480 '\r\n","convertCMD += '-VPU_NUMBER_OF_SHAVES 8 '\r\n","convertCMD += '-VPU_NUMBER_OF_CMX_SLICES 8 '\r\n","convertCMD += '-ip U8 '\r\n","convertCMD += '-op FP32'\r\n","\r\n","print(convertCMD)\r\n","!$convertCMD"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598978244502}}},{"cell_type":"code","source":["# Package up blob for delevery to devkit\r\n","\r\n","# Clean recreate directory if it exists and create it if not \r\n","!rm -rf bl!ob\r\n","mkdir blob\r\n","\r\n","!cp ssdlite_mobilenet_v2.blob blob/ssdlite_mobilenet_v2.blob\r\n","\r\n","%cd blob"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598978258187}}},{"cell_type":"code","source":["# Create labels file for devkit\r\n","# Do not remove zeroindex, it is needed to align graph classification index with lables index\r\n","labelFileName = 'labels.txt'"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598986928982}}},{"cell_type":"code","source":["%%writefile $labelFileName\r\n","zeroindex\r\n","bowl"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Fix to remove ^M nonprintable char from end of string lines in file created above\r\n","# to see issue run \"!cat -v $labelFileName\" before and after fix\r\n","with open(labelFileName,'r') as file:\r\n","    labelFile = file.read()\r\n","updateFile = open(labelFileName,\"w\")\r\n","updateFile.writelines(labelFile)\r\n","updateFile.close() "],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598978311133}}},{"cell_type":"code","source":["# Create json file for devkit\r\n","jsonConfigFileName = 'config.json'"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598986939934}}},{"cell_type":"code","source":["%%writefile $jsonConfigFileName\r\n","{\r\n","    \"DomainType\": \"ssd100\",\r\n","    \"LabelFileName\": \"labels.txt\",\r\n","    \"ModelFileName\": \"ssdlite_mobilenet_v2.blob\"\r\n","}"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Fix to remove ^M nonprintable char from end of string lines in file created above\r\n","# to see issue run \"!cat -v $jsonConfigFileName\" before and after fix\r\n","with open(jsonConfigFileName,'r') as file:\r\n","    jsonConfigFile = file.read()\r\n","updateFile = open(jsonConfigFileName,\"w\")\r\n","updateFile.writelines(jsonConfigFile)\r\n","updateFile.close() "],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598978363734}}},{"cell_type":"code","source":["# Package up model and support files for dev kit\r\n","!zip -r model.zip ./*"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598978378986}}},{"cell_type":"code","source":["#Reload workspace details for module twin update\r\n","from azureml.core import Workspace\r\n","ws.get_details()"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598986950202}}},{"cell_type":"code","source":["# Get the default datastore\r\n","ds = ws.get_default_datastore()\r\n","print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598986960142}}},{"cell_type":"code","source":["# Set data path for model.zip and upload\r\n","data_path = 'model'\r\n","ds.upload(src_dir='.', target_path=data_path, overwrite=True)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598978411086}}},{"cell_type":"code","source":["!pip install azure-storage-blob==2.1.0\n","!pip install msrest"],"outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":["#Use one of the options to generate Saas Url"],"outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":["# Option 1- Generate download SAS URL for model.zip\r\n","from datetime import datetime, timedelta\r\n","from azure.storage.blob import (\r\n","    BlockBlobService,\r\n","    ContainerPermissions,\r\n","    BlobPermissions,\r\n","    PublicAccess,\r\n",")\r\n","\r\n","AZURE_ACC_NAME = ds.account_name\r\n","AZURE_PRIMARY_KEY = ds.account_key\r\n","AZURE_CONTAINER = ds.container_name\r\n","AZURE_BLOB=ds.name\r\n","AZURE_File=data_path+'/model.zip'\r\n","\r\n","block_blob_service = BlockBlobService(account_name=AZURE_ACC_NAME, account_key=AZURE_PRIMARY_KEY)\r\n","sas_url = block_blob_service.generate_blob_shared_access_signature(AZURE_CONTAINER,AZURE_File,permission=BlobPermissions.READ,expiry= datetime.utcnow() + timedelta(hours=360))\r\n","downloadurl ='https://'+AZURE_ACC_NAME+'.blob.core.windows.net/'+AZURE_CONTAINER+'/'+AZURE_File+'?'+sas_url\r\n","print('https://'+AZURE_ACC_NAME+'.blob.core.windows.net/'+AZURE_CONTAINER+'/'+AZURE_File+'?'+sas_url)\r\n","print(sas_url)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598986973636}}},{"cell_type":"code","source":["# Option 2- Generate download SAS URL for model.zip\n","from azure.storage.blob.baseblobservice import BaseBlobService,ContainerPermissions,BlobPermissions\n","from datetime import datetime, timedelta\n","AZURE_ACC_NAME = ds.account_name\n","AZURE_PRIMARY_KEY = ds.account_key\n","AZURE_CONTAINER = ds.container_name\n","AZURE_BLOB=ds.name\n","AZURE_File=data_path+'/model.zip'\n","service = BaseBlobService(account_name=AZURE_ACC_NAME, account_key=AZURE_PRIMARY_KEY)\n","sas_url  = service.generate_blob_shared_access_signature(AZURE_CONTAINER,AZURE_File,permission=BlobPermissions.READ,expiry= datetime.utcnow() + timedelta(hours=48))\n","downloadurl ='https://'+AZURE_ACC_NAME+'.blob.core.windows.net/'+AZURE_CONTAINER+'/'+AZURE_File+'?'+sas_url\n","print('https://'+AZURE_ACC_NAME+'.blob.core.windows.net/'+AZURE_CONTAINER+'/'+AZURE_File+'?'+sas_url)\n","print(sas_url)"],"outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":["# Perform Module twin update\r\n","# Incorporate the connection string, device_id and the module_id values from your IoTHub\r\n","\r\n","#!pip install azure-iot-hubprint\r\n","print(downloadurl)\r\n","import sys\r\n","from azure.iot.hub import IoTHubRegistryManager\r\n","from azure.iot.hub.models import Twin, TwinProperties\r\n","\r\n","# Incorporate Iothub connection string and the default module name\r\n","# Go to Https://portal.azure.com\r\n","# Select your IoTHub\r\n","# Click on Shared access policies\r\n","# Click service on right\r\n","# Copy the iothub connection string primary key\r\n","\r\n","CONNECTION_STRING = '<ENTER YOUR CONNECTION STRING>'\r\n","DEVICE_ID = '<ENTER YOUR DEVICE NAME>'\r\n","MODULE_ID = \"azureeyemodule\"\r\n","\r\n","try:\r\n","    # RegistryManager\r\n","    iothub_registry_manager = IoTHubRegistryManager(CONNECTION_STRING)\r\n","\r\n","    module_twin = iothub_registry_manager.get_module_twin(DEVICE_ID, MODULE_ID)\r\n","    print ( \"\" )\r\n","    print ( \"Module twin properties before update    :\" )\r\n","    print ( \"{0}\".format(module_twin.properties) )\r\n","\r\n","    # Update twin\r\n","    twin_patch = Twin()\r\n","    twin_patch.properties = TwinProperties(desired={\"ModelZipUrl\": downloadurl})\r\n","    updated_module_twin = iothub_registry_manager.update_module_twin(\r\n","        DEVICE_ID, MODULE_ID, twin_patch, module_twin.etag\r\n","    )\r\n","    print ( \"\" )\r\n","    print ( \"Module twin properties after update     :\" )\r\n","    print ( \"{0}\".format(updated_module_twin.properties) )\r\n","\r\n","except Exception as ex:\r\n","    print ( \"Unexpected error {0}\".format(ex) )\r\n","except KeyboardInterrupt:\r\n","    print ( \"IoTHubRegistryManager sample stopped\" )"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598987982808}}},{"cell_type":"code","source":["# The trained model will get pushed to the IoT Edge device via module twin update method\r\n","# Check model inferencing by connecting monitor to the devkit or by installing VLC media player : \r\n","#Install VLC from https://www.videolan.org/vlc/ and install on “Windows” to check the camera function of “Azure Eye”.\r\n","\r\n","#Check video stream:\r\n","#1.\tSelect Media -> Open Network Stream…\r\n","#2.\tInput the network stream: “rtsp://[ip of PE-101]:8554/result” then click “Play” button.\r\n","#3. or use webstream http://<ipaddressofcamera>:3000"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}}],"metadata":{"kernelspec":{"name":"python3-azureml","language":"python","display_name":"Python 3.6 - AzureML"},"language_info":{"name":"python","version":"3.6.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"nteract":{"version":"nteract-front-end@1.0.0"},"kernel_info":{"name":"python3-azureml"}},"nbformat":4,"nbformat_minor":4}
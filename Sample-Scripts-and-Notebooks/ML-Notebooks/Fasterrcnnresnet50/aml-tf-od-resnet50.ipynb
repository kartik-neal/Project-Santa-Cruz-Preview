{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Object Detection transfer learning using tensorflow Faster_rcnn_resnet101 model on Azure ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Build tensorflow object detection docker image for AzureML\n",
    "#Specify your docker container registry (<ACR_NAME>.azurecr.io) and the repo name (dw-tf-od:v1). You can go to https://portal.azure.com and go to container registry to find your container details or create new one. \n",
    "import docker\n",
    "!docker build --tag <ACR_NAME>.azurecr.io/dw-tf-od:v1 './docker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Authenticate to container\n",
    "!az acr login -n <ACR_NAME>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Push docker image to Azure container registry which will then be used for transfer learning training\n",
    "!docker push <ACR_NAME>.azurecr.io/dw-tf-od:v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup workspace\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "print(azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting telmetry for the experiment\n",
    "#from azureml.telemetry import set_diagnostics_collection\n",
    "#set_diagnostics_collection(send_diagnostics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include your subscription, resource_group and the workspace details which will be used for experiment runs. This will create the workspace, if it doesn't exist\n",
    "subscription_id = <subscriptionid>\n",
    "resource_group = <Resoucegroup>\n",
    "workspace_name = <workspace>\n",
    "loc=<location>\n",
    "\n",
    "# create aml workspace or create it azure portal\n",
    "#https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py#workspace\n",
    "\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.create(name=workspace_name,\n",
    "                      subscription_id=subscription_id,\n",
    "                      resource_group=resource_group,\n",
    "                      create_resource_group=True,\n",
    "                      location=loc\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capture your Azure container registry details\n",
    "\n",
    "acr_server = \"<ACR_NAME.azurecr.io\"\n",
    "acr_login = \"ACR_NAME\"\n",
    "acr_pwd = \"<PWD>\"\n",
    "#acr_repo_name = \"dw-tf-od:v1-gpu\"\n",
    "acr_repo_name = \"dw-tf-od:v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload workspace details for training\n",
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup datastore for loading custom labelled datasets\n",
    "# For this training the data set was labelled using https://github.com/Microsoft/VoTT tool\n",
    "ds = ws.get_default_datastore()\n",
    "ds.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data folder to default storage datastore\n",
    "ds.upload(\n",
    "    src_dir='./upload_data',\n",
    "    target_path='tfdata',\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# train remote VM - gpu\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name='dw-gpu')\n",
    "    print('found existing:', compute_target.name)\n",
    "except ComputeTargetException:\n",
    "    print('creating new.')\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size='STANDARD_NC6',\n",
    "        min_nodes=0,\n",
    "        max_nodes=1)\n",
    "    compute_target = ComputeTarget.create(ws, 'dw-gpu', compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train remote VM - cpu\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name='dw-cpu1')\n",
    "    print('found existing:', compute_target.name)\n",
    "except ComputeTargetException:\n",
    "    print('creating new.')\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size='STANDARD_D3_V2',\n",
    "        min_nodes=0,\n",
    "        max_nodes=1)\n",
    "    compute_target = ComputeTarget.create(ws, 'dw-cpu1', compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mounting the uploaded data for training\n",
    "from azureml.core import Datastore\n",
    "from azureml.core.runconfig import DataReferenceConfiguration\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "dr_conf = DataReferenceConfiguration(\n",
    "    datastore_name=ds.name,\n",
    "    path_on_datastore='tfdata',\n",
    "    #path_on_compute = '/tfdata'\n",
    "    mode='mount') # or 'download'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Running experiment using Estimator\n",
    "# https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/distributed-cntk-with-custom-docker/distributed-cntk-with-custom-docker.ipynb\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "params= {'--data_folder' : ds.as_mount() }\n",
    "\n",
    "estimator = Estimator(source_directory='script',\n",
    "                      compute_target=compute_target,\n",
    "                      #compute_target='local',\n",
    "                      entry_script='train.py',\n",
    "                      script_params=params,\n",
    "                      node_count=1,\n",
    "                      process_count_per_node=1,\n",
    "                      pip_requirements_file = '../docker/requirements.txt', # pip packages\n",
    "                      custom_docker_image='<ACR_NAME>.azurecr.io/dw-tf-od:v1', # using public docker hub\n",
    "                      use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'dw-exp-resnet50'\n",
    "experiment = Experiment(ws, name=experiment_name)\n",
    "\n",
    "run = experiment.submit(estimator)\n",
    "print(run)\n",
    "\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#The trained model is stored from Experiment run in the Experiment -> Output+Logs tab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Reload workspace details for module twin update\n",
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Register the trained model. Once register you'll find the model in the Models section on the left pane\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(model_path = trained_model_path,\n",
    "                      model_name = \"<MyModelName>\",\n",
    "                      tags = {\"<data>\": \"<faster_rcnn_resnet50>\", \"<model>\": \"<object_detection>\", \"<type>\": \"<faster_rcnn_resnet50>\"},\n",
    "                      description = \"<Model Description>\",\n",
    "                      workspace = ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next step is to convert the convert the .pb file to .IR and .Blob format. There is already an existing converted model zip file in the outputs/convertedmodel folder that can be used to test and deploy to devkit. If you want to use this model, skip the next step on how to convert to IR/Blob and go to Reload workspace details for module twin update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "### Convert the trained model to IR -> Blob using Intel Openvino toolkit for running on Devkit running Myriadx chipset\n",
    "1) Setup Intel openvino toolkit on local machine https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_windows.html\n",
    "2) Download the Frozen_inference_graph.pb model file, pipeline.config, labels.txt and config.json from the notebook vm /outputs folder to local machine for model conversion\n",
    "3) Model conversion to IR and Blob. Run the command from the command prompt as administrator\n",
    "Pb->IR\n",
    "python \"c:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\model_optimizer\\mo_tf.py\"  --input_model frozen_inference_graph.pb --tensorflow_object_detection_api_pipeline_config pipeline.config --transformations_config   \"C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\model_optimizer\\extensions\\front\\tf\\ssd_v2_support.json\" \n",
    "\n",
    "IR->Blob\n",
    "\"C:\\Program Files (x86)\\IntelSWTools\\openvino_2020.3.194\\deployment_tools\\inference_engine\\bin\\intel64\\Release\\myriad_compile.exe\" -m frozen_inference_graph.xml -o fast-rcnn-resnet50.blob -VPU_MYRIAD_PLATFORM VPU_MYRIAD_2480 -VPU_NUMBER_OF_SHAVES 8 -VPU_NUMBER_OF_CMX_SLICES 8 -iop \"image_tensor:U8, image_info:FP32\" -op FP32\n",
    "\n",
    "4) If you trained with your custom data set, make sure to update the labels.txt with the classes used for training\n",
    "5) Zip the converted model file .blob, labels.txt and config.json to model_resnet50.zip and upload the zip file to /ouputs/convertedmodel folder in notebook vm. There is already base model zipped in the outputs\\convertedmodel folder\n",
    "6) Next step is to upload the zip file to blobstore and push it to the devkit using module twin update\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Reload workspace details for module twin update\n",
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# get the default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#set data path for model.zip\n",
    "data_path = 'modelpath'\n",
    "ds.upload(src_dir='./outputs/convertedmodel', target_path=data_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Generated Saas url for module twin update\n",
    "from azure.storage.blob.baseblobservice import BaseBlobService,BlobPermissions\n",
    "#from azure.storage.blob import BlobPermissions\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "AZURE_ACC_NAME = ds.account_name\n",
    "AZURE_PRIMARY_KEY = ds.account_key\n",
    "AZURE_CONTAINER = ds.container_name\n",
    "AZURE_BLOB=ds.name\n",
    "AZURE_File=data_path+'/model_resnet50.zip'\n",
    "\n",
    "service = BaseBlobService(account_name=AZURE_ACC_NAME, account_key=AZURE_PRIMARY_KEY)\n",
    "sas_url  = service.generate_blob_shared_access_signature(AZURE_CONTAINER,AZURE_File,permission=BlobPermissions.READ,expiry= datetime.utcnow() + timedelta(hours=48))\n",
    "#sas_url = block_blob_service.generate_blob_shared_access_signature(AZURE_CONTAINER,AZURE_File,permission=BlobPermissions.READ,expiry= datetime.utcnow() + timedelta(hours=48))\n",
    "\n",
    "#block_blob_service = BlockBlobService(account_name=AZURE_ACC_NAME, account_key=AZURE_PRIMARY_KEY)\n",
    "#sas_url = block_blob_service.generate_blob_shared_access_signature(AZURE_CONTAINER,AZURE_File,permission=BlobPermissions.READ,expiry= datetime.utcnow() + timedelta(hours=48))\n",
    "downloadurl ='https://'+AZURE_ACC_NAME+'.blob.core.windows.net/'+AZURE_CONTAINER+'/'+AZURE_File+'?'+sas_url\n",
    "print('https://'+AZURE_ACC_NAME+'.blob.core.windows.net/'+AZURE_CONTAINER+'/'+AZURE_File+'?'+sas_url)\n",
    "print(sas_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Perform Module twin update\n",
    "#Incorporate the connection string, device_id and the module_id values from your IoTHub\n",
    "\n",
    "!pip install azure-iot-hub\n",
    "import sys\n",
    "from azure.iot.hub import IoTHubRegistryManager\n",
    "from azure.iot.hub.models import Twin, TwinProperties\n",
    "\n",
    "#Incorporate Iothub connection string and the default module name\n",
    "#Go to Https://portal.azure.com\n",
    "#Select your IoTHub\n",
    "#Click on Shared access policies\n",
    "#click service on right\n",
    "#Copy the iothub connection string primary key\n",
    "#Device id is the device name\n",
    "\n",
    "CONNECTION_STRING = \"<IOTHUBConnection string>\"\n",
    "DEVICE_ID = '<Device_ID>'\n",
    "MODULE_ID = \"azureeyemodule\"\n",
    "\n",
    "try:\n",
    "    # RegistryManager\n",
    "    iothub_registry_manager = IoTHubRegistryManager(CONNECTION_STRING)\n",
    "\n",
    "    module_twin = iothub_registry_manager.get_module_twin(DEVICE_ID, MODULE_ID)\n",
    "    print ( \"\" )\n",
    "    print ( \"Module twin properties before update    :\" )\n",
    "    print ( \"{0}\".format(module_twin.properties) )\n",
    "\n",
    "    # Update twin\n",
    "    twin_patch = Twin()\n",
    "    twin_patch.properties = TwinProperties(desired={\"ModelZipUrl\": downloadurl})\n",
    "    updated_module_twin = iothub_registry_manager.update_module_twin(\n",
    "        DEVICE_ID, MODULE_ID, twin_patch, module_twin.etag\n",
    "    )\n",
    "    print ( \"\" )\n",
    "    print ( \"Module twin properties after update     :\" )\n",
    "    print ( \"{0}\".format(updated_module_twin.properties) )\n",
    "\n",
    "except Exception as ex:\n",
    "    print ( \"Unexpected error {0}\".format(ex) )\n",
    "except KeyboardInterrupt:\n",
    "    print ( \"IoTHubRegistryManager sample stopped\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# The trained model will get pushed to the IoT Edge device via module twin update method\n",
    "# Check model inferencing by connecting monitor to the devkit or by installing VLC media player : \n",
    "#Install VLC from https://www.videolan.org/vlc/ and install on “Windows” to check the camera function of “Azure Eye”.\n",
    "\n",
    "#Check video stream:\n",
    "#1.\tSelect Media -> Open Network Stream…\n",
    "#2.\tInput the network stream: “rtsp://[ip of PE-101]:8554/result” then click “Play” button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Environment Cleanup\n",
    "#Check the docker image id created for the acr repo name\n",
    "#!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Delete docker registry by uncommenting the below step and including the image id\n",
    "#!docker rmi -f <imageid>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete cpu compute\n",
    "\"\"\"\n",
    "mycompute = AmlCompute(workspace=ws, name='dw-cpu1')\n",
    "mycompute.delete()\n",
    "\n",
    "# delete gpu compute\n",
    "mycompute = AmlCompute(workspace=ws, name='dw-gpu')\n",
    "mycompute.delete()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete workspace\n",
    "#ws.delete(delete_dependent_resources=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}